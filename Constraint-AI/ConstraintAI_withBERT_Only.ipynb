{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys, string, warnings, pickle, xgboost, textblob, nltk, re\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import numpy as np\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "pd.options.display.max_colwidth = 1000\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p\n",
    "p.set_options(p.OPT.URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  (6420, 3)\n",
      "Validation Data Shape:  (2140, 3)\n",
      "Test Data Shape:  (2140, 3)\n"
     ]
    }
   ],
   "source": [
    "def getData(file):\n",
    "    tsv_file = open(file)\n",
    "    df = pd.read_csv(tsv_file, delimiter=\"\\t\")\n",
    "    return df\n",
    "\n",
    "trainFilename = \"Data/Constraint_English_Train - Sheet1.tsv\"\n",
    "validFilename = \"Data/Constraint_English_Val - Sheet1.tsv\"\n",
    "testFilename = \"Data/english_test_with_labels - Sheet1.tsv\"\n",
    "trainDF = getData(trainFilename)\n",
    "validDF = getData(validFilename)\n",
    "testDF = getData(testFilename)\n",
    "print(\"Train Data Shape: \",trainDF.shape)\n",
    "print(\"Validation Data Shape: \",validDF.shape)\n",
    "print(\"Test Data Shape: \",testDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8558, 3) (2, 3)\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.concat([trainDF, validDF])\n",
    "\n",
    "\n",
    "#Split the dataframe into train and valid\n",
    "msk = np.random.rand(len(trainDF)) < 0.9999\n",
    "\n",
    "validDF = trainDF[~msk]\n",
    "trainDF = trainDF[msk]\n",
    "\n",
    "print(trainDF.shape, validDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTweet(row):\n",
    "    text = row['tweet']\n",
    "    text = p.clean(text)\n",
    "    text = text.lower().replace(\"#\", \"\").replace(\"@\", \"\")\n",
    "    return text\n",
    "\n",
    "trainDF['processedTweet'] = trainDF.apply(preprocessTweet, axis=1)\n",
    "validDF['processedTweet'] = validDF.apply(preprocessTweet, axis=1)\n",
    "testDF['processedTweet'] = testDF.apply(preprocessTweet, axis=1)\n",
    "\n",
    "totalTextData = pd.concat([trainDF['tweet'], validDF['tweet']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalData = pd.concat([trainDF, validDF, testDF])\n",
    "realData = totalData[totalData[\"label\"]==\"real\"][\"processedTweet\"]\n",
    "fakeData = totalData[totalData[\"label\"]==\"fake\"][\"processedTweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979329\n",
      "620900\n"
     ]
    }
   ],
   "source": [
    "positiveCloud = \"\"\n",
    "for i in realData:\n",
    "    i = re.sub(r'\\b\\w{1,2}\\b', '', i)\n",
    "    i = i.split()\n",
    "    positiveCloud += \" \".join(i)+\" \"\n",
    "print(len(positiveCloud))\n",
    "\n",
    "negativeCloud = \"\"\n",
    "for i in fakeData:\n",
    "    i = re.sub(r'\\b\\w{1,2}\\b', '', i)\n",
    "    i = i.split()\n",
    "    negativeCloud += \" \".join(i)+\" \"\n",
    "print(len(negativeCloud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "positiveWordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(positiveCloud) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (10, 9), facecolor = None) \n",
    "plt.imshow(positiveWordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() \n",
    "plt.savefig('positive.png')\n",
    "\n",
    "negativeWordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white',  \n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(negativeCloud) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (10, 9), facecolor = None) \n",
    "plt.imshow(negativeWordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() \n",
    "plt.savefig('negative.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 2)\n",
      "0.9682242990654205\n",
      "0.9682203781683173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9696    0.9639    0.9668      1026\n",
      "        real     0.9670    0.9722    0.9696      1114\n",
      "\n",
      "    accuracy                         0.9682      2140\n",
      "   macro avg     0.9683    0.9681    0.9682      2140\n",
      "weighted avg     0.9682    0.9682    0.9682      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"Results/answer_withdev.txt\"\n",
    "data_X = getData(file)\n",
    "print(data_X.shape)\n",
    "data_X['label']\n",
    "\n",
    "print(metrics.accuracy_score(data_X['label'], testDF['label']))\n",
    "print(metrics.f1_score(data_X['label'], testDF['label'], average='weighted'))\n",
    "print(classification_report(data_X['label'], testDF['label'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "'''tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=15000)\n",
    "tfidf_vect.fit(totalTextData)\n",
    "xtrain_tfidf =  tfidf_vect.transform(trainDF['tweet'])\n",
    "xvalid_tfidf =  tfidf_vect.transform(validDF['tweet'])\n",
    "\n",
    "# ngram level tf-idf\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
    "tfidf_vect_ngram.fit(totalTextData)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(trainDF['tweet'])\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(validDF['tweet'])'''\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,5), max_features=100)\n",
    "tfidf_vect_ngram_chars.fit(totalTextData)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(totalTextData) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(validDF['tweet']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(testDF['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of texts:  26.946495327102802\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for item in totalTextData:\n",
    "    lengths.append(len(item.split()))\n",
    "print(\"Average length of texts: \", sum(lengths)/len(lengths))\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(totalTextData)\n",
    "word_index = token.word_index\n",
    "\n",
    "input_size=35\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(trainDF['processedTweet']), maxlen=input_size)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(validDF['processedTweet']), maxlen=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('Embeddings/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "    \n",
    "#create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#Save to pickle        \n",
    "with open('wordEmbeddingsMatrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pickle\n",
    "with open('wordEmbeddingsMatrix.pickle', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 2)\n",
      "(2140, 2)\n"
     ]
    }
   ],
   "source": [
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(['real','fake'])\n",
    "trainDF['numericalLabels'] = labelEncoder.transform(trainDF['label'])\n",
    "validDF['numericalLabels'] = labelEncoder.transform(validDF['label'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "trainLabels = encoder.fit_transform(trainDF['label'])\n",
    "trainLabels = [to_categorical(i, num_classes=2) for i in trainLabels]\n",
    "trainLabels = np.asarray(trainLabels)\n",
    "print(trainLabels.shape)\n",
    "validLabels = encoder.fit_transform(validDF['label'])\n",
    "validLabels = [to_categorical(i, num_classes=2) for i in validLabels]\n",
    "validLabels = np.asarray(validLabels)\n",
    "print(validLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, feature_vector_test, epoch=False, is_neural_net=False):\n",
    "    if is_neural_net:\n",
    "        classifier.fit(feature_vector_train, label, epochs=epoch)\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        predictions1 = predictions.argmax(axis=-1)\n",
    "        validLabels1 = validLabels.argmax(axis=-1)\n",
    "        acc = metrics.accuracy_score(predictions1, validLabels1)\n",
    "        f1Score = metrics.f1_score(predictions1, validLabels1, average='macro')\n",
    "        classificationReport = classification_report(predictions1, validLabels1)\n",
    "        \n",
    "    else:\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        validDF['predictedLabel'] = predictions\n",
    "        test_predictions = classifier.predict(feature_vector_test)\n",
    "        testDF['predictedLabel'] = test_predictions\n",
    "        acc = metrics.accuracy_score(test_predictions, testDF['label'])\n",
    "        f1Score = metrics.f1_score(test_predictions, testDF['label'], average='macro')\n",
    "        classificationReport = classification_report(test_predictions, testDF['label'], digits=4)\n",
    "    \n",
    "    return acc, f1Score, classificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classificationReport = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classificationReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [8560, 6420]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0ce985bbab44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# PassiveAggressiveClassifier on Character Level TF IDF Vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_Score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassificationReport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPassiveAggressiveClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_tfidf_ngram_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_tfidf_ngram_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtest_tfidf_ngram_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PassiveAggressiveClassifier, CharLevel Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_Score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassificationReport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-53e79fcc8e5f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, feature_vector_test, epoch, is_neural_net)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_projects/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/_passive_aggressive.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    252\u001b[0m         return self._fit(X, y, alpha=1.0, C=self.C,\n\u001b[1;32m    253\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hinge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                          coef_init=coef_init, intercept_init=intercept_init)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_projects/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",\n\u001b[0;32m--> 525\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# labels can be encoded as float, int, or string literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_projects/my_project_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my_projects/my_project_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8560, 6420]"
     ]
    }
   ],
   "source": [
    "# PassiveAggressiveClassifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classificationReport = train_model(PassiveAggressiveClassifier(max_iter=100, random_state=0,tol=2e-3), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "print(\"PassiveAggressiveClassifier, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classificationReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:  0.9182242990654206 0.9179532524141352 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.92      0.91       997\n",
      "        real       0.93      0.91      0.92      1143\n",
      "\n",
      "    accuracy                           0.92      2140\n",
      "   macro avg       0.92      0.92      0.92      2140\n",
      "weighted avg       0.92      0.92      0.92      2140\n",
      "\n",
      "LR, N-Gram Vectors:  0.905607476635514 0.9054686809788851 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.91      0.89      0.90      1038\n",
      "        real       0.90      0.92      0.91      1102\n",
      "\n",
      "    accuracy                           0.91      2140\n",
      "   macro avg       0.91      0.91      0.91      2140\n",
      "weighted avg       0.91      0.91      0.91      2140\n",
      "\n",
      "LR, CharLevel Vectors:  0.9392523364485982 0.9391715258020307 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.95      0.93      0.94      1042\n",
      "        real       0.93      0.95      0.94      1098\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.94      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(linear_model.LogisticRegression(), xtrain_tfidf, trainDF['label'], xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy, f1_Score,\"\\n\", classReport)\n",
    "\n",
    "#Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, trainDF['label'], xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy, f1_Score,\"\\n\", classReport)\n",
    "\n",
    "#Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors:  0.9560747663551402 0.9559786414565827 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9539    0.9539    0.9539      1020\n",
      "        real     0.9580    0.9580    0.9580      1120\n",
      "\n",
      "    accuracy                         0.9561      2140\n",
      "   macro avg     0.9560    0.9560    0.9560      2140\n",
      "weighted avg     0.9561    0.9561    0.9561      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''#SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(svm.SVC(), xtrain_tfidf, trainDF['label'], xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF: \", accuracy, f1_Score,\"\\n\", classReport)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(svm.SVC(), xtrain_tfidf_ngram, trainDF['label'], xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy, f1_Score,\"\\n\", classReport)'''\n",
    "\n",
    "#SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "print(\"SVM, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id,label\n",
      "1,real\n",
      "2,fake\n",
      "3,fake\n",
      "4,real\n",
      "5,real\n",
      "6,real\n",
      "7,real\n",
      "8,fake\n",
      "9,real\n",
      "10,real\n",
      "11,fake\n",
      "12,fake\n",
      "13,real\n",
      "14,fake\n",
      "15,fake\n",
      "16,fake\n",
      "17,fake\n",
      "18,fake\n",
      "19,real\n",
      "20,real\n",
      "21,fake\n",
      "22,fake\n",
      "23,real\n",
      "24,fake\n",
      "25,real\n",
      "26,real\n",
      "27,fake\n",
      "28,fake\n",
      "29,real\n",
      "30,fake\n",
      "31,fake\n",
      "32,fake\n",
      "33,real\n",
      "34,fake\n",
      "35,real\n",
      "36,real\n",
      "37,real\n",
      "38,real\n",
      "39,fake\n",
      "40,real\n",
      "41,real\n",
      "42,fake\n",
      "43,fake\n",
      "44,fake\n",
      "45,fake\n",
      "46,real\n",
      "47,real\n",
      "48,fake\n",
      "49,fake\n",
      "50,real\n",
      "51,fake\n",
      "52,fake\n",
      "53,fake\n",
      "54,fake\n",
      "55,fake\n",
      "56,fake\n",
      "57,real\n",
      "58,fake\n",
      "59,real\n",
      "60,real\n",
      "61,fake\n",
      "62,real\n",
      "63,fake\n",
      "64,real\n",
      "65,fake\n",
      "66,fake\n",
      "67,real\n",
      "68,fake\n",
      "69,real\n",
      "70,real\n",
      "71,real\n",
      "72,fake\n",
      "73,fake\n",
      "74,real\n",
      "75,real\n",
      "76,real\n",
      "77,real\n",
      "78,real\n",
      "79,fake\n",
      "80,fake\n",
      "81,real\n",
      "82,fake\n",
      "83,real\n",
      "84,real\n",
      "85,real\n",
      "86,fake\n",
      "87,real\n",
      "88,real\n",
      "89,real\n",
      "90,fake\n",
      "91,real\n",
      "92,fake\n",
      "93,fake\n",
      "94,fake\n",
      "95,fake\n",
      "96,real\n",
      "97,fake\n",
      "98,real\n",
      "99,real\n",
      "100,real\n",
      "101,real\n",
      "102,fake\n",
      "103,fake\n",
      "104,real\n",
      "105,fake\n",
      "106,real\n",
      "107,real\n",
      "108,real\n",
      "109,real\n",
      "110,real\n",
      "111,real\n",
      "112,fake\n",
      "113,real\n",
      "114,fake\n",
      "115,real\n",
      "116,real\n",
      "117,fake\n",
      "118,real\n",
      "119,fake\n",
      "120,fake\n",
      "121,real\n",
      "122,real\n",
      "123,fake\n",
      "124,real\n",
      "125,real\n",
      "126,real\n",
      "127,real\n",
      "128,real\n",
      "129,fake\n",
      "130,real\n",
      "131,real\n",
      "132,fake\n",
      "133,real\n",
      "134,fake\n",
      "135,fake\n",
      "136,fake\n",
      "137,fake\n",
      "138,real\n",
      "139,fake\n",
      "140,real\n",
      "141,real\n",
      "142,fake\n",
      "143,real\n",
      "144,fake\n",
      "145,real\n",
      "146,real\n",
      "147,real\n",
      "148,real\n",
      "149,real\n",
      "150,real\n",
      "151,real\n",
      "152,real\n",
      "153,fake\n",
      "154,fake\n",
      "155,fake\n",
      "156,fake\n",
      "157,real\n",
      "158,real\n",
      "159,fake\n",
      "160,fake\n",
      "161,fake\n",
      "162,real\n",
      "163,real\n",
      "164,real\n",
      "165,fake\n",
      "166,real\n",
      "167,real\n",
      "168,fake\n",
      "169,fake\n",
      "170,fake\n",
      "171,real\n",
      "172,real\n",
      "173,real\n",
      "174,real\n",
      "175,fake\n",
      "176,real\n",
      "177,fake\n",
      "178,fake\n",
      "179,real\n",
      "180,real\n",
      "181,fake\n",
      "182,real\n",
      "183,real\n",
      "184,fake\n",
      "185,fake\n",
      "186,fake\n",
      "187,fake\n",
      "188,real\n",
      "189,fake\n",
      "190,fake\n",
      "191,fake\n",
      "192,real\n",
      "193,real\n",
      "194,real\n",
      "195,real\n",
      "196,fake\n",
      "197,real\n",
      "198,real\n",
      "199,real\n",
      "200,fake\n",
      "201,fake\n",
      "202,real\n",
      "203,fake\n",
      "204,fake\n",
      "205,real\n",
      "206,fake\n",
      "207,real\n",
      "208,real\n",
      "209,fake\n",
      "210,real\n",
      "211,real\n",
      "212,fake\n",
      "213,fake\n",
      "214,fake\n",
      "215,real\n",
      "216,fake\n",
      "217,fake\n",
      "218,fake\n",
      "219,real\n",
      "220,fake\n",
      "221,real\n",
      "222,real\n",
      "223,real\n",
      "224,real\n",
      "225,fake\n",
      "226,real\n",
      "227,fake\n",
      "228,real\n",
      "229,fake\n",
      "230,real\n",
      "231,real\n",
      "232,fake\n",
      "233,real\n",
      "234,real\n",
      "235,fake\n",
      "236,real\n",
      "237,fake\n",
      "238,fake\n",
      "239,real\n",
      "240,fake\n",
      "241,fake\n",
      "242,fake\n",
      "243,real\n",
      "244,real\n",
      "245,fake\n",
      "246,fake\n",
      "247,real\n",
      "248,real\n",
      "249,real\n",
      "250,fake\n",
      "251,real\n",
      "252,real\n",
      "253,fake\n",
      "254,real\n",
      "255,fake\n",
      "256,fake\n",
      "257,fake\n",
      "258,fake\n",
      "259,fake\n",
      "260,real\n",
      "261,real\n",
      "262,real\n",
      "263,fake\n",
      "264,fake\n",
      "265,real\n",
      "266,fake\n",
      "267,fake\n",
      "268,fake\n",
      "269,real\n",
      "270,real\n",
      "271,fake\n",
      "272,real\n",
      "273,real\n",
      "274,fake\n",
      "275,real\n",
      "276,real\n",
      "277,fake\n",
      "278,real\n",
      "279,fake\n",
      "280,real\n",
      "281,fake\n",
      "282,fake\n",
      "283,fake\n",
      "284,fake\n",
      "285,real\n",
      "286,fake\n",
      "287,real\n",
      "288,fake\n",
      "289,real\n",
      "290,real\n",
      "291,fake\n",
      "292,fake\n",
      "293,real\n",
      "294,real\n",
      "295,fake\n",
      "296,real\n",
      "297,fake\n",
      "298,fake\n",
      "299,real\n",
      "300,real\n",
      "301,real\n",
      "302,fake\n",
      "303,fake\n",
      "304,real\n",
      "305,real\n",
      "306,real\n",
      "307,fake\n",
      "308,fake\n",
      "309,real\n",
      "310,real\n",
      "311,fake\n",
      "312,real\n",
      "313,fake\n",
      "314,fake\n",
      "315,real\n",
      "316,fake\n",
      "317,fake\n",
      "318,fake\n",
      "319,real\n",
      "320,real\n",
      "321,fake\n",
      "322,real\n",
      "323,real\n",
      "324,real\n",
      "325,real\n",
      "326,fake\n",
      "327,real\n",
      "328,fake\n",
      "329,fake\n",
      "330,fake\n",
      "331,fake\n",
      "332,real\n",
      "333,fake\n",
      "334,fake\n",
      "335,fake\n",
      "336,fake\n",
      "337,fake\n",
      "338,fake\n",
      "339,fake\n",
      "340,real\n",
      "341,real\n",
      "342,real\n",
      "343,real\n",
      "344,fake\n",
      "345,real\n",
      "346,fake\n",
      "347,fake\n",
      "348,fake\n",
      "349,real\n",
      "350,real\n",
      "351,real\n",
      "352,fake\n",
      "353,fake\n",
      "354,fake\n",
      "355,real\n",
      "356,fake\n",
      "357,fake\n",
      "358,real\n",
      "359,fake\n",
      "360,real\n",
      "361,real\n",
      "362,real\n",
      "363,real\n",
      "364,fake\n",
      "365,fake\n",
      "366,real\n",
      "367,fake\n",
      "368,fake\n",
      "369,real\n",
      "370,fake\n",
      "371,fake\n",
      "372,real\n",
      "373,real\n",
      "374,real\n",
      "375,real\n",
      "376,real\n",
      "377,real\n",
      "378,real\n",
      "379,fake\n",
      "380,real\n",
      "381,real\n",
      "382,real\n",
      "383,real\n",
      "384,real\n",
      "385,fake\n",
      "386,fake\n",
      "387,real\n",
      "388,real\n",
      "389,real\n",
      "390,fake\n",
      "391,real\n",
      "392,real\n",
      "393,real\n",
      "394,fake\n",
      "395,fake\n",
      "396,fake\n",
      "397,fake\n",
      "398,fake\n",
      "399,fake\n",
      "400,fake\n",
      "401,real\n",
      "402,fake\n",
      "403,fake\n",
      "404,real\n",
      "405,real\n",
      "406,real\n",
      "407,real\n",
      "408,fake\n",
      "409,fake\n",
      "410,real\n",
      "411,real\n",
      "412,real\n",
      "413,real\n",
      "414,real\n",
      "415,real\n",
      "416,fake\n",
      "417,real\n",
      "418,fake\n",
      "419,real\n",
      "420,real\n",
      "421,real\n",
      "422,real\n",
      "423,real\n",
      "424,real\n",
      "425,real\n",
      "426,fake\n",
      "427,fake\n",
      "428,real\n",
      "429,fake\n",
      "430,real\n",
      "431,fake\n",
      "432,real\n",
      "433,real\n",
      "434,real\n",
      "435,fake\n",
      "436,fake\n",
      "437,real\n",
      "438,real\n",
      "439,real\n",
      "440,real\n",
      "441,real\n",
      "442,real\n",
      "443,fake\n",
      "444,fake\n",
      "445,real\n",
      "446,real\n",
      "447,fake\n",
      "448,fake\n",
      "449,fake\n",
      "450,fake\n",
      "451,real\n",
      "452,real\n",
      "453,fake\n",
      "454,real\n",
      "455,real\n",
      "456,fake\n",
      "457,real\n",
      "458,fake\n",
      "459,fake\n",
      "460,fake\n",
      "461,fake\n",
      "462,fake\n",
      "463,fake\n",
      "464,fake\n",
      "465,real\n",
      "466,fake\n",
      "467,real\n",
      "468,real\n",
      "469,real\n",
      "470,real\n",
      "471,fake\n",
      "472,real\n",
      "473,fake\n",
      "474,fake\n",
      "475,real\n",
      "476,fake\n",
      "477,fake\n",
      "478,fake\n",
      "479,fake\n",
      "480,real\n",
      "481,fake\n",
      "482,real\n",
      "483,fake\n",
      "484,fake\n",
      "485,real\n",
      "486,fake\n",
      "487,fake\n",
      "488,real\n",
      "489,real\n",
      "490,fake\n",
      "491,fake\n",
      "492,fake\n",
      "493,fake\n",
      "494,fake\n",
      "495,fake\n",
      "496,real\n",
      "497,fake\n",
      "498,fake\n",
      "499,fake\n",
      "500,real\n",
      "501,real\n",
      "502,fake\n",
      "503,fake\n",
      "504,fake\n",
      "505,fake\n",
      "506,real\n",
      "507,fake\n",
      "508,real\n",
      "509,fake\n",
      "510,fake\n",
      "511,fake\n",
      "512,fake\n",
      "513,fake\n",
      "514,real\n",
      "515,real\n",
      "516,real\n",
      "517,real\n",
      "518,fake\n",
      "519,fake\n",
      "520,real\n",
      "521,fake\n",
      "522,real\n",
      "523,fake\n",
      "524,real\n",
      "525,real\n",
      "526,fake\n",
      "527,real\n",
      "528,real\n",
      "529,fake\n",
      "530,fake\n",
      "531,fake\n",
      "532,real\n",
      "533,real\n",
      "534,real\n",
      "535,real\n",
      "536,fake\n",
      "537,fake\n",
      "538,real\n",
      "539,real\n",
      "540,fake\n",
      "541,real\n",
      "542,real\n",
      "543,real\n",
      "544,fake\n",
      "545,fake\n",
      "546,fake\n",
      "547,fake\n",
      "548,fake\n",
      "549,real\n",
      "550,real\n",
      "551,fake\n",
      "552,real\n",
      "553,fake\n",
      "554,fake\n",
      "555,real\n",
      "556,fake\n",
      "557,fake\n",
      "558,fake\n",
      "559,real\n",
      "560,real\n",
      "561,real\n",
      "562,fake\n",
      "563,fake\n",
      "564,real\n",
      "565,real\n",
      "566,real\n",
      "567,real\n",
      "568,fake\n",
      "569,fake\n",
      "570,fake\n",
      "571,fake\n",
      "572,real\n",
      "573,real\n",
      "574,fake\n",
      "575,real\n",
      "576,real\n",
      "577,fake\n",
      "578,fake\n",
      "579,fake\n",
      "580,fake\n",
      "581,fake\n",
      "582,real\n",
      "583,fake\n",
      "584,real\n",
      "585,real\n",
      "586,real\n",
      "587,real\n",
      "588,fake\n",
      "589,real\n",
      "590,real\n",
      "591,fake\n",
      "592,real\n",
      "593,real\n",
      "594,real\n",
      "595,fake\n",
      "596,real\n",
      "597,real\n",
      "598,real\n",
      "599,fake\n",
      "600,real\n",
      "601,fake\n",
      "602,fake\n",
      "603,real\n",
      "604,fake\n",
      "605,fake\n",
      "606,real\n",
      "607,fake\n",
      "608,fake\n",
      "609,fake\n",
      "610,real\n",
      "611,fake\n",
      "612,real\n",
      "613,fake\n",
      "614,fake\n",
      "615,real\n",
      "616,real\n",
      "617,real\n",
      "618,real\n",
      "619,fake\n",
      "620,fake\n",
      "621,fake\n",
      "622,real\n",
      "623,fake\n",
      "624,real\n",
      "625,fake\n",
      "626,real\n",
      "627,fake\n",
      "628,fake\n",
      "629,real\n",
      "630,real\n",
      "631,real\n",
      "632,fake\n",
      "633,fake\n",
      "634,fake\n",
      "635,real\n",
      "636,real\n",
      "637,real\n",
      "638,real\n",
      "639,real\n",
      "640,fake\n",
      "641,fake\n",
      "642,fake\n",
      "643,fake\n",
      "644,fake\n",
      "645,real\n",
      "646,fake\n",
      "647,fake\n",
      "648,real\n",
      "649,real\n",
      "650,fake\n",
      "651,real\n",
      "652,fake\n",
      "653,fake\n",
      "654,real\n",
      "655,real\n",
      "656,fake\n",
      "657,real\n",
      "658,fake\n",
      "659,fake\n",
      "660,fake\n",
      "661,fake\n",
      "662,fake\n",
      "663,real\n",
      "664,fake\n",
      "665,real\n",
      "666,real\n",
      "667,fake\n",
      "668,fake\n",
      "669,real\n",
      "670,fake\n",
      "671,real\n",
      "672,real\n",
      "673,real\n",
      "674,fake\n",
      "675,real\n",
      "676,real\n",
      "677,fake\n",
      "678,fake\n",
      "679,real\n",
      "680,fake\n",
      "681,real\n",
      "682,real\n",
      "683,real\n",
      "684,real\n",
      "685,fake\n",
      "686,real\n",
      "687,real\n",
      "688,fake\n",
      "689,fake\n",
      "690,fake\n",
      "691,real\n",
      "692,real\n",
      "693,fake\n",
      "694,fake\n",
      "695,real\n",
      "696,fake\n",
      "697,real\n",
      "698,real\n",
      "699,real\n",
      "700,fake\n",
      "701,real\n",
      "702,real\n",
      "703,real\n",
      "704,real\n",
      "705,real\n",
      "706,fake\n",
      "707,fake\n",
      "708,real\n",
      "709,real\n",
      "710,fake\n",
      "711,real\n",
      "712,fake\n",
      "713,real\n",
      "714,fake\n",
      "715,fake\n",
      "716,real\n",
      "717,fake\n",
      "718,fake\n",
      "719,fake\n",
      "720,fake\n",
      "721,fake\n",
      "722,real\n",
      "723,real\n",
      "724,real\n",
      "725,fake\n",
      "726,real\n",
      "727,real\n",
      "728,fake\n",
      "729,fake\n",
      "730,real\n",
      "731,fake\n",
      "732,real\n",
      "733,real\n",
      "734,real\n",
      "735,fake\n",
      "736,fake\n",
      "737,fake\n",
      "738,fake\n",
      "739,fake\n",
      "740,real\n",
      "741,real\n",
      "742,real\n",
      "743,fake\n",
      "744,fake\n",
      "745,fake\n",
      "746,fake\n",
      "747,real\n",
      "748,fake\n",
      "749,real\n",
      "750,real\n",
      "751,fake\n",
      "752,real\n",
      "753,fake\n",
      "754,real\n",
      "755,real\n",
      "756,real\n",
      "757,fake\n",
      "758,fake\n",
      "759,fake\n",
      "760,fake\n",
      "761,real\n",
      "762,real\n",
      "763,real\n",
      "764,real\n",
      "765,fake\n",
      "766,real\n",
      "767,real\n",
      "768,fake\n",
      "769,fake\n",
      "770,real\n",
      "771,real\n",
      "772,real\n",
      "773,fake\n",
      "774,fake\n",
      "775,fake\n",
      "776,real\n",
      "777,real\n",
      "778,real\n",
      "779,real\n",
      "780,real\n",
      "781,real\n",
      "782,fake\n",
      "783,fake\n",
      "784,real\n",
      "785,real\n",
      "786,real\n",
      "787,fake\n",
      "788,real\n",
      "789,real\n",
      "790,real\n",
      "791,real\n",
      "792,fake\n",
      "793,real\n",
      "794,real\n",
      "795,fake\n",
      "796,real\n",
      "797,fake\n",
      "798,real\n",
      "799,fake\n",
      "800,real\n",
      "801,real\n",
      "802,real\n",
      "803,fake\n",
      "804,fake\n",
      "805,fake\n",
      "806,fake\n",
      "807,real\n",
      "808,real\n",
      "809,real\n",
      "810,fake\n",
      "811,fake\n",
      "812,fake\n",
      "813,fake\n",
      "814,real\n",
      "815,real\n",
      "816,fake\n",
      "817,fake\n",
      "818,fake\n",
      "819,real\n",
      "820,real\n",
      "821,fake\n",
      "822,real\n",
      "823,fake\n",
      "824,real\n",
      "825,fake\n",
      "826,real\n",
      "827,real\n",
      "828,fake\n",
      "829,fake\n",
      "830,fake\n",
      "831,fake\n",
      "832,fake\n",
      "833,fake\n",
      "834,real\n",
      "835,fake\n",
      "836,fake\n",
      "837,fake\n",
      "838,fake\n",
      "839,real\n",
      "840,fake\n",
      "841,real\n",
      "842,real\n",
      "843,fake\n",
      "844,real\n",
      "845,real\n",
      "846,fake\n",
      "847,fake\n",
      "848,fake\n",
      "849,real\n",
      "850,real\n",
      "851,fake\n",
      "852,fake\n",
      "853,fake\n",
      "854,real\n",
      "855,real\n",
      "856,real\n",
      "857,real\n",
      "858,real\n",
      "859,fake\n",
      "860,real\n",
      "861,real\n",
      "862,fake\n",
      "863,fake\n",
      "864,real\n",
      "865,real\n",
      "866,fake\n",
      "867,real\n",
      "868,real\n",
      "869,real\n",
      "870,fake\n",
      "871,real\n",
      "872,real\n",
      "873,real\n",
      "874,real\n",
      "875,real\n",
      "876,real\n",
      "877,real\n",
      "878,real\n",
      "879,real\n",
      "880,real\n",
      "881,fake\n",
      "882,real\n",
      "883,fake\n",
      "884,real\n",
      "885,real\n",
      "886,real\n",
      "887,fake\n",
      "888,fake\n",
      "889,real\n",
      "890,fake\n",
      "891,fake\n",
      "892,real\n",
      "893,fake\n",
      "894,real\n",
      "895,real\n",
      "896,fake\n",
      "897,fake\n",
      "898,real\n",
      "899,real\n",
      "900,fake\n",
      "901,real\n",
      "902,fake\n",
      "903,fake\n",
      "904,real\n",
      "905,real\n",
      "906,fake\n",
      "907,fake\n",
      "908,fake\n",
      "909,fake\n",
      "910,fake\n",
      "911,fake\n",
      "912,real\n",
      "913,fake\n",
      "914,fake\n",
      "915,real\n",
      "916,real\n",
      "917,fake\n",
      "918,fake\n",
      "919,real\n",
      "920,real\n",
      "921,fake\n",
      "922,fake\n",
      "923,real\n",
      "924,real\n",
      "925,fake\n",
      "926,real\n",
      "927,fake\n",
      "928,real\n",
      "929,real\n",
      "930,fake\n",
      "931,real\n",
      "932,real\n",
      "933,fake\n",
      "934,fake\n",
      "935,real\n",
      "936,real\n",
      "937,real\n",
      "938,real\n",
      "939,real\n",
      "940,fake\n",
      "941,fake\n",
      "942,real\n",
      "943,fake\n",
      "944,fake\n",
      "945,real\n",
      "946,real\n",
      "947,real\n",
      "948,fake\n",
      "949,fake\n",
      "950,fake\n",
      "951,fake\n",
      "952,fake\n",
      "953,fake\n",
      "954,real\n",
      "955,fake\n",
      "956,real\n",
      "957,fake\n",
      "958,fake\n",
      "959,real\n",
      "960,real\n",
      "961,fake\n",
      "962,real\n",
      "963,fake\n",
      "964,fake\n",
      "965,real\n",
      "966,real\n",
      "967,fake\n",
      "968,real\n",
      "969,real\n",
      "970,fake\n",
      "971,real\n",
      "972,fake\n",
      "973,fake\n",
      "974,fake\n",
      "975,real\n",
      "976,fake\n",
      "977,real\n",
      "978,fake\n",
      "979,real\n",
      "980,fake\n",
      "981,fake\n",
      "982,fake\n",
      "983,fake\n",
      "984,real\n",
      "985,fake\n",
      "986,real\n",
      "987,real\n",
      "988,fake\n",
      "989,fake\n",
      "990,real\n",
      "991,fake\n",
      "992,fake\n",
      "993,fake\n",
      "994,fake\n",
      "995,fake\n",
      "996,real\n",
      "997,fake\n",
      "998,real\n",
      "999,real\n",
      "1000,fake\n",
      "1001,real\n",
      "1002,fake\n",
      "1003,real\n",
      "1004,fake\n",
      "1005,fake\n",
      "1006,fake\n",
      "1007,real\n",
      "1008,fake\n",
      "1009,real\n",
      "1010,real\n",
      "1011,fake\n",
      "1012,real\n",
      "1013,real\n",
      "1014,fake\n",
      "1015,fake\n",
      "1016,fake\n",
      "1017,real\n",
      "1018,fake\n",
      "1019,real\n",
      "1020,fake\n",
      "1021,real\n",
      "1022,real\n",
      "1023,real\n",
      "1024,fake\n",
      "1025,fake\n",
      "1026,fake\n",
      "1027,real\n",
      "1028,fake\n",
      "1029,real\n",
      "1030,real\n",
      "1031,real\n",
      "1032,real\n",
      "1033,fake\n",
      "1034,real\n",
      "1035,fake\n",
      "1036,real\n",
      "1037,fake\n",
      "1038,real\n",
      "1039,fake\n",
      "1040,fake\n",
      "1041,real\n",
      "1042,fake\n",
      "1043,fake\n",
      "1044,real\n",
      "1045,fake\n",
      "1046,real\n",
      "1047,real\n",
      "1048,fake\n",
      "1049,real\n",
      "1050,real\n",
      "1051,real\n",
      "1052,real\n",
      "1053,fake\n",
      "1054,real\n",
      "1055,real\n",
      "1056,real\n",
      "1057,real\n",
      "1058,fake\n",
      "1059,real\n",
      "1060,fake\n",
      "1061,fake\n",
      "1062,real\n",
      "1063,fake\n",
      "1064,fake\n",
      "1065,fake\n",
      "1066,real\n",
      "1067,fake\n",
      "1068,real\n",
      "1069,real\n",
      "1070,real\n",
      "1071,real\n",
      "1072,real\n",
      "1073,fake\n",
      "1074,fake\n",
      "1075,fake\n",
      "1076,real\n",
      "1077,real\n",
      "1078,fake\n",
      "1079,real\n",
      "1080,real\n",
      "1081,real\n",
      "1082,fake\n",
      "1083,real\n",
      "1084,real\n",
      "1085,fake\n",
      "1086,fake\n",
      "1087,fake\n",
      "1088,fake\n",
      "1089,real\n",
      "1090,fake\n",
      "1091,real\n",
      "1092,real\n",
      "1093,real\n",
      "1094,real\n",
      "1095,fake\n",
      "1096,real\n",
      "1097,real\n",
      "1098,fake\n",
      "1099,real\n",
      "1100,fake\n",
      "1101,real\n",
      "1102,fake\n",
      "1103,real\n",
      "1104,real\n",
      "1105,real\n",
      "1106,real\n",
      "1107,fake\n",
      "1108,fake\n",
      "1109,fake\n",
      "1110,fake\n",
      "1111,real\n",
      "1112,fake\n",
      "1113,real\n",
      "1114,fake\n",
      "1115,real\n",
      "1116,real\n",
      "1117,fake\n",
      "1118,fake\n",
      "1119,fake\n",
      "1120,real\n",
      "1121,real\n",
      "1122,fake\n",
      "1123,real\n",
      "1124,real\n",
      "1125,fake\n",
      "1126,fake\n",
      "1127,fake\n",
      "1128,real\n",
      "1129,fake\n",
      "1130,fake\n",
      "1131,fake\n",
      "1132,fake\n",
      "1133,real\n",
      "1134,fake\n",
      "1135,fake\n",
      "1136,fake\n",
      "1137,fake\n",
      "1138,real\n",
      "1139,fake\n",
      "1140,real\n",
      "1141,fake\n",
      "1142,real\n",
      "1143,fake\n",
      "1144,real\n",
      "1145,fake\n",
      "1146,real\n",
      "1147,real\n",
      "1148,fake\n",
      "1149,real\n",
      "1150,fake\n",
      "1151,real\n",
      "1152,fake\n",
      "1153,fake\n",
      "1154,fake\n",
      "1155,real\n",
      "1156,fake\n",
      "1157,real\n",
      "1158,fake\n",
      "1159,real\n",
      "1160,real\n",
      "1161,fake\n",
      "1162,fake\n",
      "1163,real\n",
      "1164,real\n",
      "1165,real\n",
      "1166,real\n",
      "1167,real\n",
      "1168,real\n",
      "1169,real\n",
      "1170,fake\n",
      "1171,real\n",
      "1172,fake\n",
      "1173,fake\n",
      "1174,fake\n",
      "1175,real\n",
      "1176,real\n",
      "1177,fake\n",
      "1178,real\n",
      "1179,fake\n",
      "1180,real\n",
      "1181,fake\n",
      "1182,fake\n",
      "1183,fake\n",
      "1184,real\n",
      "1185,real\n",
      "1186,real\n",
      "1187,fake\n",
      "1188,real\n",
      "1189,fake\n",
      "1190,real\n",
      "1191,fake\n",
      "1192,fake\n",
      "1193,real\n",
      "1194,fake\n",
      "1195,real\n",
      "1196,fake\n",
      "1197,fake\n",
      "1198,fake\n",
      "1199,fake\n",
      "1200,fake\n",
      "1201,real\n",
      "1202,real\n",
      "1203,real\n",
      "1204,fake\n",
      "1205,fake\n",
      "1206,real\n",
      "1207,real\n",
      "1208,fake\n",
      "1209,real\n",
      "1210,fake\n",
      "1211,real\n",
      "1212,real\n",
      "1213,fake\n",
      "1214,real\n",
      "1215,real\n",
      "1216,real\n",
      "1217,real\n",
      "1218,real\n",
      "1219,fake\n",
      "1220,real\n",
      "1221,real\n",
      "1222,fake\n",
      "1223,fake\n",
      "1224,real\n",
      "1225,real\n",
      "1226,fake\n",
      "1227,real\n",
      "1228,real\n",
      "1229,real\n",
      "1230,fake\n",
      "1231,fake\n",
      "1232,fake\n",
      "1233,fake\n",
      "1234,real\n",
      "1235,real\n",
      "1236,real\n",
      "1237,fake\n",
      "1238,fake\n",
      "1239,real\n",
      "1240,real\n",
      "1241,fake\n",
      "1242,real\n",
      "1243,real\n",
      "1244,fake\n",
      "1245,fake\n",
      "1246,fake\n",
      "1247,fake\n",
      "1248,real\n",
      "1249,real\n",
      "1250,real\n",
      "1251,real\n",
      "1252,fake\n",
      "1253,real\n",
      "1254,fake\n",
      "1255,real\n",
      "1256,fake\n",
      "1257,fake\n",
      "1258,fake\n",
      "1259,fake\n",
      "1260,real\n",
      "1261,real\n",
      "1262,fake\n",
      "1263,real\n",
      "1264,fake\n",
      "1265,real\n",
      "1266,fake\n",
      "1267,real\n",
      "1268,real\n",
      "1269,fake\n",
      "1270,fake\n",
      "1271,real\n",
      "1272,fake\n",
      "1273,real\n",
      "1274,fake\n",
      "1275,real\n",
      "1276,real\n",
      "1277,real\n",
      "1278,fake\n",
      "1279,real\n",
      "1280,real\n",
      "1281,fake\n",
      "1282,real\n",
      "1283,fake\n",
      "1284,fake\n",
      "1285,fake\n",
      "1286,real\n",
      "1287,fake\n",
      "1288,fake\n",
      "1289,fake\n",
      "1290,fake\n",
      "1291,fake\n",
      "1292,real\n",
      "1293,fake\n",
      "1294,fake\n",
      "1295,fake\n",
      "1296,real\n",
      "1297,real\n",
      "1298,fake\n",
      "1299,fake\n",
      "1300,real\n",
      "1301,real\n",
      "1302,real\n",
      "1303,real\n",
      "1304,real\n",
      "1305,real\n",
      "1306,fake\n",
      "1307,fake\n",
      "1308,real\n",
      "1309,real\n",
      "1310,real\n",
      "1311,fake\n",
      "1312,real\n",
      "1313,real\n",
      "1314,real\n",
      "1315,fake\n",
      "1316,real\n",
      "1317,fake\n",
      "1318,real\n",
      "1319,fake\n",
      "1320,real\n",
      "1321,fake\n",
      "1322,fake\n",
      "1323,real\n",
      "1324,fake\n",
      "1325,real\n",
      "1326,real\n",
      "1327,real\n",
      "1328,real\n",
      "1329,real\n",
      "1330,fake\n",
      "1331,fake\n",
      "1332,fake\n",
      "1333,fake\n",
      "1334,fake\n",
      "1335,real\n",
      "1336,fake\n",
      "1337,fake\n",
      "1338,fake\n",
      "1339,real\n",
      "1340,real\n",
      "1341,fake\n",
      "1342,fake\n",
      "1343,fake\n",
      "1344,real\n",
      "1345,fake\n",
      "1346,real\n",
      "1347,real\n",
      "1348,real\n",
      "1349,fake\n",
      "1350,real\n",
      "1351,real\n",
      "1352,real\n",
      "1353,real\n",
      "1354,real\n",
      "1355,real\n",
      "1356,real\n",
      "1357,fake\n",
      "1358,real\n",
      "1359,fake\n",
      "1360,real\n",
      "1361,real\n",
      "1362,fake\n",
      "1363,real\n",
      "1364,fake\n",
      "1365,fake\n",
      "1366,fake\n",
      "1367,fake\n",
      "1368,fake\n",
      "1369,fake\n",
      "1370,real\n",
      "1371,fake\n",
      "1372,fake\n",
      "1373,real\n",
      "1374,fake\n",
      "1375,real\n",
      "1376,fake\n",
      "1377,real\n",
      "1378,real\n",
      "1379,real\n",
      "1380,real\n",
      "1381,fake\n",
      "1382,fake\n",
      "1383,real\n",
      "1384,real\n",
      "1385,fake\n",
      "1386,fake\n",
      "1387,fake\n",
      "1388,fake\n",
      "1389,real\n",
      "1390,real\n",
      "1391,real\n",
      "1392,real\n",
      "1393,fake\n",
      "1394,fake\n",
      "1395,real\n",
      "1396,real\n",
      "1397,real\n",
      "1398,fake\n",
      "1399,fake\n",
      "1400,fake\n",
      "1401,fake\n",
      "1402,fake\n",
      "1403,fake\n",
      "1404,fake\n",
      "1405,real\n",
      "1406,real\n",
      "1407,fake\n",
      "1408,real\n",
      "1409,fake\n",
      "1410,fake\n",
      "1411,real\n",
      "1412,real\n",
      "1413,fake\n",
      "1414,real\n",
      "1415,real\n",
      "1416,real\n",
      "1417,fake\n",
      "1418,real\n",
      "1419,fake\n",
      "1420,real\n",
      "1421,real\n",
      "1422,fake\n",
      "1423,real\n",
      "1424,real\n",
      "1425,real\n",
      "1426,real\n",
      "1427,fake\n",
      "1428,real\n",
      "1429,fake\n",
      "1430,real\n",
      "1431,real\n",
      "1432,fake\n",
      "1433,real\n",
      "1434,real\n",
      "1435,real\n",
      "1436,real\n",
      "1437,fake\n",
      "1438,real\n",
      "1439,fake\n",
      "1440,real\n",
      "1441,fake\n",
      "1442,real\n",
      "1443,fake\n",
      "1444,fake\n",
      "1445,fake\n",
      "1446,real\n",
      "1447,fake\n",
      "1448,fake\n",
      "1449,fake\n",
      "1450,real\n",
      "1451,fake\n",
      "1452,fake\n",
      "1453,real\n",
      "1454,fake\n",
      "1455,real\n",
      "1456,real\n",
      "1457,fake\n",
      "1458,fake\n",
      "1459,real\n",
      "1460,real\n",
      "1461,real\n",
      "1462,fake\n",
      "1463,real\n",
      "1464,real\n",
      "1465,real\n",
      "1466,real\n",
      "1467,real\n",
      "1468,real\n",
      "1469,real\n",
      "1470,real\n",
      "1471,fake\n",
      "1472,real\n",
      "1473,fake\n",
      "1474,fake\n",
      "1475,real\n",
      "1476,fake\n",
      "1477,fake\n",
      "1478,real\n",
      "1479,real\n",
      "1480,fake\n",
      "1481,real\n",
      "1482,real\n",
      "1483,fake\n",
      "1484,fake\n",
      "1485,real\n",
      "1486,real\n",
      "1487,fake\n",
      "1488,fake\n",
      "1489,real\n",
      "1490,real\n",
      "1491,real\n",
      "1492,real\n",
      "1493,real\n",
      "1494,real\n",
      "1495,fake\n",
      "1496,fake\n",
      "1497,fake\n",
      "1498,real\n",
      "1499,real\n",
      "1500,fake\n",
      "1501,real\n",
      "1502,real\n",
      "1503,fake\n",
      "1504,fake\n",
      "1505,real\n",
      "1506,real\n",
      "1507,real\n",
      "1508,fake\n",
      "1509,fake\n",
      "1510,real\n",
      "1511,real\n",
      "1512,real\n",
      "1513,real\n",
      "1514,real\n",
      "1515,fake\n",
      "1516,real\n",
      "1517,real\n",
      "1518,real\n",
      "1519,fake\n",
      "1520,real\n",
      "1521,real\n",
      "1522,real\n",
      "1523,fake\n",
      "1524,fake\n",
      "1525,real\n",
      "1526,fake\n",
      "1527,real\n",
      "1528,fake\n",
      "1529,fake\n",
      "1530,fake\n",
      "1531,fake\n",
      "1532,fake\n",
      "1533,real\n",
      "1534,fake\n",
      "1535,real\n",
      "1536,fake\n",
      "1537,fake\n",
      "1538,fake\n",
      "1539,fake\n",
      "1540,real\n",
      "1541,real\n",
      "1542,fake\n",
      "1543,real\n",
      "1544,real\n",
      "1545,fake\n",
      "1546,real\n",
      "1547,real\n",
      "1548,real\n",
      "1549,fake\n",
      "1550,real\n",
      "1551,fake\n",
      "1552,real\n",
      "1553,real\n",
      "1554,fake\n",
      "1555,real\n",
      "1556,fake\n",
      "1557,fake\n",
      "1558,real\n",
      "1559,real\n",
      "1560,fake\n",
      "1561,real\n",
      "1562,real\n",
      "1563,fake\n",
      "1564,real\n",
      "1565,fake\n",
      "1566,real\n",
      "1567,real\n",
      "1568,fake\n",
      "1569,fake\n",
      "1570,real\n",
      "1571,fake\n",
      "1572,real\n",
      "1573,fake\n",
      "1574,real\n",
      "1575,fake\n",
      "1576,real\n",
      "1577,real\n",
      "1578,real\n",
      "1579,fake\n",
      "1580,fake\n",
      "1581,real\n",
      "1582,fake\n",
      "1583,real\n",
      "1584,real\n",
      "1585,fake\n",
      "1586,fake\n",
      "1587,fake\n",
      "1588,real\n",
      "1589,fake\n",
      "1590,fake\n",
      "1591,real\n",
      "1592,fake\n",
      "1593,fake\n",
      "1594,fake\n",
      "1595,real\n",
      "1596,real\n",
      "1597,real\n",
      "1598,fake\n",
      "1599,fake\n",
      "1600,fake\n",
      "1601,fake\n",
      "1602,real\n",
      "1603,fake\n",
      "1604,fake\n",
      "1605,real\n",
      "1606,fake\n",
      "1607,fake\n",
      "1608,real\n",
      "1609,real\n",
      "1610,fake\n",
      "1611,real\n",
      "1612,real\n",
      "1613,fake\n",
      "1614,fake\n",
      "1615,fake\n",
      "1616,real\n",
      "1617,real\n",
      "1618,fake\n",
      "1619,fake\n",
      "1620,fake\n",
      "1621,fake\n",
      "1622,real\n",
      "1623,fake\n",
      "1624,fake\n",
      "1625,fake\n",
      "1626,real\n",
      "1627,real\n",
      "1628,real\n",
      "1629,real\n",
      "1630,real\n",
      "1631,real\n",
      "1632,fake\n",
      "1633,real\n",
      "1634,real\n",
      "1635,real\n",
      "1636,real\n",
      "1637,real\n",
      "1638,real\n",
      "1639,real\n",
      "1640,fake\n",
      "1641,real\n",
      "1642,real\n",
      "1643,fake\n",
      "1644,fake\n",
      "1645,real\n",
      "1646,fake\n",
      "1647,real\n",
      "1648,real\n",
      "1649,fake\n",
      "1650,real\n",
      "1651,real\n",
      "1652,real\n",
      "1653,real\n",
      "1654,fake\n",
      "1655,real\n",
      "1656,fake\n",
      "1657,fake\n",
      "1658,real\n",
      "1659,fake\n",
      "1660,fake\n",
      "1661,real\n",
      "1662,fake\n",
      "1663,real\n",
      "1664,real\n",
      "1665,fake\n",
      "1666,real\n",
      "1667,real\n",
      "1668,fake\n",
      "1669,real\n",
      "1670,fake\n",
      "1671,real\n",
      "1672,real\n",
      "1673,fake\n",
      "1674,real\n",
      "1675,real\n",
      "1676,fake\n",
      "1677,fake\n",
      "1678,fake\n",
      "1679,real\n",
      "1680,real\n",
      "1681,fake\n",
      "1682,real\n",
      "1683,real\n",
      "1684,real\n",
      "1685,fake\n",
      "1686,real\n",
      "1687,fake\n",
      "1688,fake\n",
      "1689,real\n",
      "1690,fake\n",
      "1691,real\n",
      "1692,real\n",
      "1693,fake\n",
      "1694,real\n",
      "1695,fake\n",
      "1696,fake\n",
      "1697,real\n",
      "1698,fake\n",
      "1699,fake\n",
      "1700,fake\n",
      "1701,real\n",
      "1702,fake\n",
      "1703,fake\n",
      "1704,fake\n",
      "1705,real\n",
      "1706,fake\n",
      "1707,real\n",
      "1708,real\n",
      "1709,real\n",
      "1710,fake\n",
      "1711,real\n",
      "1712,fake\n",
      "1713,real\n",
      "1714,real\n",
      "1715,fake\n",
      "1716,fake\n",
      "1717,fake\n",
      "1718,fake\n",
      "1719,real\n",
      "1720,real\n",
      "1721,real\n",
      "1722,fake\n",
      "1723,fake\n",
      "1724,real\n",
      "1725,real\n",
      "1726,real\n",
      "1727,real\n",
      "1728,real\n",
      "1729,fake\n",
      "1730,real\n",
      "1731,real\n",
      "1732,fake\n",
      "1733,real\n",
      "1734,fake\n",
      "1735,fake\n",
      "1736,real\n",
      "1737,real\n",
      "1738,real\n",
      "1739,fake\n",
      "1740,real\n",
      "1741,real\n",
      "1742,real\n",
      "1743,fake\n",
      "1744,real\n",
      "1745,fake\n",
      "1746,real\n",
      "1747,real\n",
      "1748,real\n",
      "1749,real\n",
      "1750,fake\n",
      "1751,fake\n",
      "1752,fake\n",
      "1753,real\n",
      "1754,real\n",
      "1755,fake\n",
      "1756,fake\n",
      "1757,real\n",
      "1758,real\n",
      "1759,real\n",
      "1760,fake\n",
      "1761,real\n",
      "1762,fake\n",
      "1763,fake\n",
      "1764,real\n",
      "1765,fake\n",
      "1766,fake\n",
      "1767,fake\n",
      "1768,fake\n",
      "1769,fake\n",
      "1770,fake\n",
      "1771,fake\n",
      "1772,real\n",
      "1773,fake\n",
      "1774,real\n",
      "1775,real\n",
      "1776,real\n",
      "1777,fake\n",
      "1778,real\n",
      "1779,real\n",
      "1780,fake\n",
      "1781,fake\n",
      "1782,real\n",
      "1783,real\n",
      "1784,fake\n",
      "1785,fake\n",
      "1786,real\n",
      "1787,real\n",
      "1788,fake\n",
      "1789,fake\n",
      "1790,fake\n",
      "1791,fake\n",
      "1792,real\n",
      "1793,real\n",
      "1794,real\n",
      "1795,real\n",
      "1796,real\n",
      "1797,real\n",
      "1798,fake\n",
      "1799,fake\n",
      "1800,real\n",
      "1801,real\n",
      "1802,real\n",
      "1803,real\n",
      "1804,real\n",
      "1805,fake\n",
      "1806,real\n",
      "1807,fake\n",
      "1808,real\n",
      "1809,fake\n",
      "1810,real\n",
      "1811,fake\n",
      "1812,real\n",
      "1813,real\n",
      "1814,fake\n",
      "1815,real\n",
      "1816,fake\n",
      "1817,real\n",
      "1818,real\n",
      "1819,fake\n",
      "1820,real\n",
      "1821,real\n",
      "1822,fake\n",
      "1823,real\n",
      "1824,fake\n",
      "1825,fake\n",
      "1826,fake\n",
      "1827,real\n",
      "1828,real\n",
      "1829,fake\n",
      "1830,fake\n",
      "1831,fake\n",
      "1832,fake\n",
      "1833,fake\n",
      "1834,real\n",
      "1835,fake\n",
      "1836,fake\n",
      "1837,real\n",
      "1838,real\n",
      "1839,fake\n",
      "1840,fake\n",
      "1841,fake\n",
      "1842,real\n",
      "1843,real\n",
      "1844,fake\n",
      "1845,fake\n",
      "1846,real\n",
      "1847,real\n",
      "1848,real\n",
      "1849,fake\n",
      "1850,real\n",
      "1851,real\n",
      "1852,real\n",
      "1853,fake\n",
      "1854,fake\n",
      "1855,real\n",
      "1856,real\n",
      "1857,real\n",
      "1858,fake\n",
      "1859,fake\n",
      "1860,real\n",
      "1861,real\n",
      "1862,real\n",
      "1863,real\n",
      "1864,real\n",
      "1865,real\n",
      "1866,fake\n",
      "1867,real\n",
      "1868,real\n",
      "1869,fake\n",
      "1870,fake\n",
      "1871,fake\n",
      "1872,real\n",
      "1873,fake\n",
      "1874,fake\n",
      "1875,real\n",
      "1876,real\n",
      "1877,fake\n",
      "1878,real\n",
      "1879,real\n",
      "1880,fake\n",
      "1881,fake\n",
      "1882,real\n",
      "1883,real\n",
      "1884,fake\n",
      "1885,real\n",
      "1886,real\n",
      "1887,real\n",
      "1888,fake\n",
      "1889,real\n",
      "1890,fake\n",
      "1891,real\n",
      "1892,fake\n",
      "1893,fake\n",
      "1894,real\n",
      "1895,real\n",
      "1896,real\n",
      "1897,fake\n",
      "1898,real\n",
      "1899,real\n",
      "1900,fake\n",
      "1901,fake\n",
      "1902,fake\n",
      "1903,real\n",
      "1904,real\n",
      "1905,real\n",
      "1906,real\n",
      "1907,fake\n",
      "1908,real\n",
      "1909,fake\n",
      "1910,real\n",
      "1911,real\n",
      "1912,fake\n",
      "1913,real\n",
      "1914,fake\n",
      "1915,real\n",
      "1916,fake\n",
      "1917,real\n",
      "1918,real\n",
      "1919,real\n",
      "1920,fake\n",
      "1921,fake\n",
      "1922,real\n",
      "1923,real\n",
      "1924,fake\n",
      "1925,fake\n",
      "1926,fake\n",
      "1927,fake\n",
      "1928,fake\n",
      "1929,fake\n",
      "1930,fake\n",
      "1931,real\n",
      "1932,real\n",
      "1933,real\n",
      "1934,fake\n",
      "1935,real\n",
      "1936,real\n",
      "1937,real\n",
      "1938,fake\n",
      "1939,real\n",
      "1940,fake\n",
      "1941,fake\n",
      "1942,real\n",
      "1943,fake\n",
      "1944,real\n",
      "1945,real\n",
      "1946,real\n",
      "1947,real\n",
      "1948,real\n",
      "1949,fake\n",
      "1950,real\n",
      "1951,real\n",
      "1952,real\n",
      "1953,real\n",
      "1954,real\n",
      "1955,real\n",
      "1956,fake\n",
      "1957,real\n",
      "1958,fake\n",
      "1959,fake\n",
      "1960,real\n",
      "1961,fake\n",
      "1962,real\n",
      "1963,real\n",
      "1964,real\n",
      "1965,real\n",
      "1966,real\n",
      "1967,fake\n",
      "1968,real\n",
      "1969,fake\n",
      "1970,real\n",
      "1971,fake\n",
      "1972,real\n",
      "1973,real\n",
      "1974,real\n",
      "1975,real\n",
      "1976,real\n",
      "1977,real\n",
      "1978,fake\n",
      "1979,real\n",
      "1980,real\n",
      "1981,real\n",
      "1982,real\n",
      "1983,real\n",
      "1984,fake\n",
      "1985,fake\n",
      "1986,fake\n",
      "1987,fake\n",
      "1988,real\n",
      "1989,fake\n",
      "1990,fake\n",
      "1991,fake\n",
      "1992,real\n",
      "1993,real\n",
      "1994,fake\n",
      "1995,fake\n",
      "1996,fake\n",
      "1997,fake\n",
      "1998,fake\n",
      "1999,fake\n",
      "2000,fake\n",
      "2001,real\n",
      "2002,fake\n",
      "2003,fake\n",
      "2004,fake\n",
      "2005,real\n",
      "2006,real\n",
      "2007,fake\n",
      "2008,real\n",
      "2009,real\n",
      "2010,real\n",
      "2011,fake\n",
      "2012,real\n",
      "2013,real\n",
      "2014,fake\n",
      "2015,fake\n",
      "2016,fake\n",
      "2017,fake\n",
      "2018,fake\n",
      "2019,real\n",
      "2020,real\n",
      "2021,fake\n",
      "2022,real\n",
      "2023,real\n",
      "2024,real\n",
      "2025,real\n",
      "2026,fake\n",
      "2027,real\n",
      "2028,fake\n",
      "2029,fake\n",
      "2030,real\n",
      "2031,fake\n",
      "2032,real\n",
      "2033,real\n",
      "2034,fake\n",
      "2035,real\n",
      "2036,real\n",
      "2037,fake\n",
      "2038,real\n",
      "2039,fake\n",
      "2040,real\n",
      "2041,fake\n",
      "2042,real\n",
      "2043,real\n",
      "2044,fake\n",
      "2045,real\n",
      "2046,real\n",
      "2047,fake\n",
      "2048,fake\n",
      "2049,real\n",
      "2050,fake\n",
      "2051,fake\n",
      "2052,real\n",
      "2053,real\n",
      "2054,real\n",
      "2055,real\n",
      "2056,real\n",
      "2057,fake\n",
      "2058,real\n",
      "2059,real\n",
      "2060,real\n",
      "2061,real\n",
      "2062,fake\n",
      "2063,fake\n",
      "2064,real\n",
      "2065,real\n",
      "2066,real\n",
      "2067,fake\n",
      "2068,fake\n",
      "2069,fake\n",
      "2070,real\n",
      "2071,real\n",
      "2072,real\n",
      "2073,real\n",
      "2074,real\n",
      "2075,real\n",
      "2076,fake\n",
      "2077,fake\n",
      "2078,fake\n",
      "2079,fake\n",
      "2080,fake\n",
      "2081,real\n",
      "2082,fake\n",
      "2083,fake\n",
      "2084,fake\n",
      "2085,real\n",
      "2086,real\n",
      "2087,fake\n",
      "2088,fake\n",
      "2089,real\n",
      "2090,real\n",
      "2091,real\n",
      "2092,fake\n",
      "2093,fake\n",
      "2094,fake\n",
      "2095,fake\n",
      "2096,real\n",
      "2097,fake\n",
      "2098,real\n",
      "2099,fake\n",
      "2100,real\n",
      "2101,fake\n",
      "2102,fake\n",
      "2103,real\n",
      "2104,real\n",
      "2105,real\n",
      "2106,fake\n",
      "2107,fake\n",
      "2108,fake\n",
      "2109,fake\n",
      "2110,real\n",
      "2111,fake\n",
      "2112,fake\n",
      "2113,real\n",
      "2114,fake\n",
      "2115,real\n",
      "2116,fake\n",
      "2117,real\n",
      "2118,fake\n",
      "2119,fake\n",
      "2120,real\n",
      "2121,real\n",
      "2122,fake\n",
      "2123,real\n",
      "2124,fake\n",
      "2125,real\n",
      "2126,real\n",
      "2127,real\n",
      "2128,real\n",
      "2129,fake\n",
      "2130,real\n",
      "2131,real\n",
      "2132,real\n",
      "2133,fake\n",
      "2134,real\n",
      "2135,fake\n",
      "2136,real\n",
      "2137,fake\n",
      "2138,real\n",
      "2139,real\n",
      "2140,real\n"
     ]
    }
   ],
   "source": [
    "newValidDF = testDF[['id', 'predictedLabel']].copy()\n",
    "newValidDF.rename(columns = {'predictedLabel':'label'}, inplace = True) \n",
    "\n",
    "np.savetxt('answer1.txt', newValidDF.values, fmt='%s', delimiter=\",\", header=\"id,label\") \n",
    "\n",
    "f = open('answer1.txt', 'r')\n",
    "f = f.readlines()\n",
    "f1 = open('answer_svmwithoutdev.txt', 'w')\n",
    "for item in f:\n",
    "    item = item.rstrip()\n",
    "    print(item)\n",
    "    f1.write(item+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), trainDF['label'], xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \",  accuracy, f1_Score,\"\\n\", classReport)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), trainDF['label'], xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \",  accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n",
      "Iteration 3, loss = 0.46495883\n",
      "Iteration 4, loss = 0.39008737\n",
      "Iteration 5, loss = 0.32990803\n",
      "Iteration 6, loss = 0.28165034\n",
      "Iteration 7, loss = 0.24284524\n",
      "Iteration 8, loss = 0.21128739\n",
      "Iteration 9, loss = 0.18538673\n",
      "Iteration 10, loss = 0.16393060\n",
      "Iteration 11, loss = 0.14592127\n",
      "Iteration 12, loss = 0.13075963\n",
      "Iteration 13, loss = 0.11761370\n",
      "Iteration 14, loss = 0.10635901\n",
      "Iteration 15, loss = 0.09634314\n",
      "Iteration 16, loss = 0.08774038\n",
      "Iteration 17, loss = 0.08007642\n",
      "Iteration 18, loss = 0.07334565\n",
      "Iteration 19, loss = 0.06731865\n",
      "Iteration 20, loss = 0.06191068\n",
      "Iteration 21, loss = 0.05711950\n",
      "Iteration 22, loss = 0.05275508\n",
      "Iteration 23, loss = 0.04881000\n",
      "Iteration 24, loss = 0.04526325\n",
      "Iteration 25, loss = 0.04212315\n",
      "Iteration 26, loss = 0.03921543\n",
      "Iteration 27, loss = 0.03653368\n",
      "Iteration 28, loss = 0.03415443\n",
      "Iteration 29, loss = 0.03197063\n",
      "Iteration 30, loss = 0.02999023\n",
      "Iteration 31, loss = 0.02817891\n",
      "Iteration 32, loss = 0.02651755\n",
      "Iteration 33, loss = 0.02499074\n",
      "Iteration 34, loss = 0.02358934\n",
      "Iteration 35, loss = 0.02229552\n",
      "Iteration 36, loss = 0.02111128\n",
      "Iteration 37, loss = 0.02000879\n",
      "Iteration 38, loss = 0.01899576\n",
      "Iteration 39, loss = 0.01806292\n",
      "Iteration 40, loss = 0.01719316\n",
      "MLP, WordLevel TF-IDF:  0.9598130841121495 0.959735396533271 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9608    0.9552    0.9580      1026\n",
      "        real     0.9589    0.9641    0.9615      1114\n",
      "\n",
      "    accuracy                         0.9598      2140\n",
      "   macro avg     0.9599    0.9596    0.9597      2140\n",
      "weighted avg     0.9598    0.9598    0.9598      2140\n",
      "\n",
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n",
      "Iteration 3, loss = 0.46495883\n",
      "Iteration 4, loss = 0.39008737\n",
      "Iteration 5, loss = 0.32990803\n",
      "Iteration 6, loss = 0.28165034\n",
      "Iteration 7, loss = 0.24284524\n",
      "Iteration 8, loss = 0.21128739\n",
      "Iteration 9, loss = 0.18538673\n",
      "Iteration 10, loss = 0.16393060\n",
      "Iteration 11, loss = 0.14592127\n",
      "Iteration 12, loss = 0.13075963\n",
      "Iteration 13, loss = 0.11761370\n",
      "Iteration 14, loss = 0.10635901\n",
      "Iteration 15, loss = 0.09634314\n",
      "Iteration 16, loss = 0.08774038\n",
      "Iteration 17, loss = 0.08007642\n",
      "Iteration 18, loss = 0.07334565\n",
      "Iteration 19, loss = 0.06731865\n",
      "Iteration 20, loss = 0.06191068\n",
      "Iteration 21, loss = 0.05711950\n",
      "Iteration 22, loss = 0.05275508\n",
      "Iteration 23, loss = 0.04881000\n",
      "Iteration 24, loss = 0.04526325\n",
      "Iteration 25, loss = 0.04212315\n",
      "Iteration 26, loss = 0.03921543\n",
      "Iteration 27, loss = 0.03653368\n",
      "Iteration 28, loss = 0.03415443\n",
      "Iteration 29, loss = 0.03197063\n",
      "Iteration 30, loss = 0.02999023\n",
      "Iteration 31, loss = 0.02817891\n",
      "Iteration 32, loss = 0.02651755\n",
      "Iteration 33, loss = 0.02499074\n",
      "Iteration 34, loss = 0.02358934\n",
      "Iteration 35, loss = 0.02229552\n",
      "Iteration 36, loss = 0.02111128\n",
      "Iteration 37, loss = 0.02000879\n",
      "Iteration 38, loss = 0.01899576\n",
      "Iteration 39, loss = 0.01806292\n",
      "Iteration 40, loss = 0.01719316\n",
      "Iteration 41, loss = 0.01639667\n",
      "MLP, WordLevel TF-IDF:  0.9602803738317757 0.9601986000147478 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9598    0.9570    0.9584      1023\n",
      "        real     0.9607    0.9633    0.9620      1117\n",
      "\n",
      "    accuracy                         0.9603      2140\n",
      "   macro avg     0.9603    0.9601    0.9602      2140\n",
      "weighted avg     0.9603    0.9603    0.9603      2140\n",
      "\n",
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n",
      "Iteration 3, loss = 0.46495883\n",
      "Iteration 4, loss = 0.39008737\n",
      "Iteration 5, loss = 0.32990803\n",
      "Iteration 6, loss = 0.28165034\n",
      "Iteration 7, loss = 0.24284524\n",
      "Iteration 8, loss = 0.21128739\n",
      "Iteration 9, loss = 0.18538673\n",
      "Iteration 10, loss = 0.16393060\n",
      "Iteration 11, loss = 0.14592127\n",
      "Iteration 12, loss = 0.13075963\n",
      "Iteration 13, loss = 0.11761370\n",
      "Iteration 14, loss = 0.10635901\n",
      "Iteration 15, loss = 0.09634314\n",
      "Iteration 16, loss = 0.08774038\n",
      "Iteration 17, loss = 0.08007642\n",
      "Iteration 18, loss = 0.07334565\n",
      "Iteration 19, loss = 0.06731865\n",
      "Iteration 20, loss = 0.06191068\n",
      "Iteration 21, loss = 0.05711950\n",
      "Iteration 22, loss = 0.05275508\n",
      "Iteration 23, loss = 0.04881000\n",
      "Iteration 24, loss = 0.04526325\n",
      "Iteration 25, loss = 0.04212315\n",
      "Iteration 26, loss = 0.03921543\n",
      "Iteration 27, loss = 0.03653368\n",
      "Iteration 28, loss = 0.03415443\n",
      "Iteration 29, loss = 0.03197063\n",
      "Iteration 30, loss = 0.02999023\n",
      "Iteration 31, loss = 0.02817891\n",
      "Iteration 32, loss = 0.02651755\n",
      "Iteration 33, loss = 0.02499074\n",
      "Iteration 34, loss = 0.02358934\n",
      "Iteration 35, loss = 0.02229552\n",
      "Iteration 36, loss = 0.02111128\n",
      "Iteration 37, loss = 0.02000879\n",
      "Iteration 38, loss = 0.01899576\n",
      "Iteration 39, loss = 0.01806292\n",
      "Iteration 40, loss = 0.01719316\n",
      "Iteration 41, loss = 0.01639667\n",
      "Iteration 42, loss = 0.01565260\n",
      "MLP, WordLevel TF-IDF:  0.9602803738317757 0.9602019439567231 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9608    0.9561    0.9584      1025\n",
      "        real     0.9598    0.9641    0.9620      1115\n",
      "\n",
      "    accuracy                         0.9603      2140\n",
      "   macro avg     0.9603    0.9601    0.9602      2140\n",
      "weighted avg     0.9603    0.9603    0.9603      2140\n",
      "\n",
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n",
      "Iteration 3, loss = 0.46495883\n",
      "Iteration 4, loss = 0.39008737\n",
      "Iteration 5, loss = 0.32990803\n",
      "Iteration 6, loss = 0.28165034\n",
      "Iteration 7, loss = 0.24284524\n",
      "Iteration 8, loss = 0.21128739\n",
      "Iteration 9, loss = 0.18538673\n",
      "Iteration 10, loss = 0.16393060\n",
      "Iteration 11, loss = 0.14592127\n",
      "Iteration 12, loss = 0.13075963\n",
      "Iteration 13, loss = 0.11761370\n",
      "Iteration 14, loss = 0.10635901\n",
      "Iteration 15, loss = 0.09634314\n",
      "Iteration 16, loss = 0.08774038\n",
      "Iteration 17, loss = 0.08007642\n",
      "Iteration 18, loss = 0.07334565\n",
      "Iteration 19, loss = 0.06731865\n",
      "Iteration 20, loss = 0.06191068\n",
      "Iteration 21, loss = 0.05711950\n",
      "Iteration 22, loss = 0.05275508\n",
      "Iteration 23, loss = 0.04881000\n",
      "Iteration 24, loss = 0.04526325\n",
      "Iteration 25, loss = 0.04212315\n",
      "Iteration 26, loss = 0.03921543\n",
      "Iteration 27, loss = 0.03653368\n",
      "Iteration 28, loss = 0.03415443\n",
      "Iteration 29, loss = 0.03197063\n",
      "Iteration 30, loss = 0.02999023\n",
      "Iteration 31, loss = 0.02817891\n",
      "Iteration 32, loss = 0.02651755\n",
      "Iteration 33, loss = 0.02499074\n",
      "Iteration 34, loss = 0.02358934\n",
      "Iteration 35, loss = 0.02229552\n",
      "Iteration 36, loss = 0.02111128\n",
      "Iteration 37, loss = 0.02000879\n",
      "Iteration 38, loss = 0.01899576\n",
      "Iteration 39, loss = 0.01806292\n",
      "Iteration 40, loss = 0.01719316\n",
      "Iteration 41, loss = 0.01639667\n",
      "Iteration 42, loss = 0.01565260\n",
      "Iteration 43, loss = 0.01495224\n",
      "MLP, WordLevel TF-IDF:  0.9602803738317757 0.9602019439567231 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9608    0.9561    0.9584      1025\n",
      "        real     0.9598    0.9641    0.9620      1115\n",
      "\n",
      "    accuracy                         0.9603      2140\n",
      "   macro avg     0.9603    0.9601    0.9602      2140\n",
      "weighted avg     0.9603    0.9603    0.9603      2140\n",
      "\n",
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n",
      "Iteration 3, loss = 0.46495883\n",
      "Iteration 4, loss = 0.39008737\n",
      "Iteration 5, loss = 0.32990803\n",
      "Iteration 6, loss = 0.28165034\n",
      "Iteration 7, loss = 0.24284524\n",
      "Iteration 8, loss = 0.21128739\n",
      "Iteration 9, loss = 0.18538673\n",
      "Iteration 10, loss = 0.16393060\n",
      "Iteration 11, loss = 0.14592127\n",
      "Iteration 12, loss = 0.13075963\n",
      "Iteration 13, loss = 0.11761370\n",
      "Iteration 14, loss = 0.10635901\n",
      "Iteration 15, loss = 0.09634314\n",
      "Iteration 16, loss = 0.08774038\n",
      "Iteration 17, loss = 0.08007642\n",
      "Iteration 18, loss = 0.07334565\n",
      "Iteration 19, loss = 0.06731865\n",
      "Iteration 20, loss = 0.06191068\n",
      "Iteration 21, loss = 0.05711950\n",
      "Iteration 22, loss = 0.05275508\n",
      "Iteration 23, loss = 0.04881000\n",
      "Iteration 24, loss = 0.04526325\n",
      "Iteration 25, loss = 0.04212315\n",
      "Iteration 26, loss = 0.03921543\n",
      "Iteration 27, loss = 0.03653368\n",
      "Iteration 28, loss = 0.03415443\n",
      "Iteration 29, loss = 0.03197063\n",
      "Iteration 30, loss = 0.02999023\n",
      "Iteration 31, loss = 0.02817891\n",
      "Iteration 32, loss = 0.02651755\n",
      "Iteration 33, loss = 0.02499074\n",
      "Iteration 34, loss = 0.02358934\n",
      "Iteration 35, loss = 0.02229552\n",
      "Iteration 36, loss = 0.02111128\n",
      "Iteration 37, loss = 0.02000879\n",
      "Iteration 38, loss = 0.01899576\n",
      "Iteration 39, loss = 0.01806292\n",
      "Iteration 40, loss = 0.01719316\n",
      "Iteration 41, loss = 0.01639667\n",
      "Iteration 42, loss = 0.01565260\n",
      "Iteration 43, loss = 0.01495224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44, loss = 0.01432071\n",
      "MLP, WordLevel TF-IDF:  0.9602803738317757 0.9602019439567231 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake     0.9608    0.9561    0.9584      1025\n",
      "        real     0.9598    0.9641    0.9620      1115\n",
      "\n",
      "    accuracy                         0.9603      2140\n",
      "   macro avg     0.9603    0.9601    0.9602      2140\n",
      "weighted avg     0.9603    0.9603    0.9603      2140\n",
      "\n",
      "Iteration 1, loss = 0.65511344\n",
      "Iteration 2, loss = 0.55498717\n"
     ]
    }
   ],
   "source": [
    "for i in range(40,55):\n",
    "    # Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "    accuracy, f1_Score, classReport = train_model(MLPClassifier(hidden_layer_sizes= (256,), learning_rate_init=0.0001, activation='relu', verbose=True, random_state=1, max_iter=i), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "    print(\"MLP, WordLevel TF-IDF: \",  accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "201/201 [==============================] - 6s 30ms/step - loss: 0.3032\n",
      "Epoch 2/6\n",
      "201/201 [==============================] - 6s 27ms/step - loss: 0.1029\n",
      "Epoch 3/6\n",
      "201/201 [==============================] - 6s 29ms/step - loss: 0.0305\n",
      "Epoch 4/6\n",
      "201/201 [==============================] - 6s 28ms/step - loss: 0.0120\n",
      "Epoch 5/6\n",
      "201/201 [==============================] - 6s 27ms/step - loss: 0.0020\n",
      "Epoch 6/6\n",
      "201/201 [==============================] - 6s 28ms/step - loss: 5.0401e-04\n",
      "CNN, Glove:  0.9425233644859813 0.9424192448612753 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      1029\n",
      "           1       0.94      0.95      0.94      1111\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.94      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    #embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(512, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"relu\")(pooling_layer)\n",
    "    #output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.001), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn(input_size)\n",
    "accuracy, f1_Score, classReport = train_model(classifier, train_seq_x, trainLabels, valid_seq_x, 6, is_neural_net=True)\n",
    "print(\"CNN, Glove: \",  accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "201/201 [==============================] - 20s 100ms/step - loss: 0.4459\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 21s 104ms/step - loss: 0.2420\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 23s 115ms/step - loss: 0.1691\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 22s 111ms/step - loss: 0.1191\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 21s 106ms/step - loss: 0.0863\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 19s 96ms/step - loss: 0.0543\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 21s 107ms/step - loss: 0.0423\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 22s 109ms/step - loss: 0.0259\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 22s 109ms/step - loss: 0.0141\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 19s 94ms/step - loss: 0.0302\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 19s 93ms/step - loss: 0.0277\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 18s 87ms/step - loss: 0.0174\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 18s 91ms/step - loss: 0.0192\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 22s 109ms/step - loss: 0.0148\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 19s 96ms/step - loss: 0.0148\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 18s 87ms/step - loss: 0.0219\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 19s 93ms/step - loss: 0.0097\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 20s 99ms/step - loss: 0.0024\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 20s 99ms/step - loss: 0.0030\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 19s 95ms/step - loss: 0.0101\n",
      "RNN-LSTM, Word Embeddings 0.914018691588785 0.9139212524615165 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      1048\n",
      "           1       0.91      0.93      0.92      1092\n",
      "\n",
      "    accuracy                           0.91      2140\n",
      "   macro avg       0.91      0.91      0.91      2140\n",
      "weighted avg       0.91      0.91      0.91      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm(input_size):    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    #embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(256)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"relu\")(lstm_layer)\n",
    "    #output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.01), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm(input_size)\n",
    "accuracy, f1_Score, classReport = train_model(classifier, train_seq_x, trainLabels, valid_seq_x, 20, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy, f1_Score, \"\\n\", classReport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
